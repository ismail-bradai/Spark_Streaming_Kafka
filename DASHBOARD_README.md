# ğŸš€ Spark Streaming Kafka with Streamlit Dashboard

## ğŸ“Š Complete Real-Time Visualization Pipeline

A production-ready **end-to-end streaming data pipeline** with real-time visualization:

```
Producer â†’ Kafka â†’ Delta Lake Bronze â†’ Silver Transformations â†’ Streamlit Dashboard
```

---

## âœ¨ Key Features

### ğŸ”´ Real-Time Data Pipeline
- **Producer**: Generates 100+ sales transactions per minute (every 2 seconds)
- **Kafka**: Message broker with auto-partitioning
- **Delta Lake**: ACID transactions with versioning and time-travel
- **Spark Streaming**: Continuous processing with watermarking
- **Silver Layer**: Cleaned and transformed data
- **Streamlit**: Interactive live dashboard

### ğŸ“ˆ Dashboard Visualizations
1. **Real-time Metrics** (5 KPIs updated every 5 seconds)
   - Total Records
   - Unique Clients
   - Unique Products
   - Total Revenue
   - Average Transaction

2. **Sales by Country** - Bar chart showing revenue per country
3. **Top 10 Products** - Best sellers ranked by quantity
4. **Sales Timeline** - Line chart of sales velocity over time
5. **Raw Data** - Paginated Bronze layer transactions
6. **System Architecture** - Pipeline diagram and monitoring commands

### âš™ï¸ Configuration & Monitoring
- **Auto-refresh** from 5-30 seconds (configurable)
- **Real-time alerts** for pipeline health
- **System architecture** diagram embedded in dashboard
- **Copy-paste commands** for monitoring logs
- **Live data exploration** without code

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Producer (Python)                                â”‚
â”‚  - Generates sales transactions (JSON)                  â”‚
â”‚  - 1 message every 2 seconds (~30/min)                  â”‚
â”‚  - Random customers, products, amounts                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Message: {"vente_id", "client_id", ...}
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Kafka (localhost:9092)                          â”‚
â”‚  - Topic: ventes_stream                                 â”‚
â”‚  - 1 partition, 1 replica                               â”‚
â”‚  - Message rate: ~30/min                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Kafka Consumer
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark Streaming Job #1 (spark_streaming_delta.py)      â”‚
â”‚  - Kafka â†’ Delta Lake (Bronze)                          â”‚
â”‚  - Watermark: 10 minutes late arrivals                  â”‚
â”‚  - Partition by: jour (transaction date)                â”‚
â”‚  - Checkpoint: /tmp/delta/checkpoints/                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ writeStream() to Bronze
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Delta Lake Bronze Layer (/tmp/delta/bronze/)           â”‚
â”‚  - Raw, 1:1 copy from Kafka                             â”‚
â”‚  - Format: Parquet + Delta transactions                 â”‚
â”‚  - ACID guarantees, versioning enabled                  â”‚
â”‚  - ~581+ parquet files (grows with data)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ readStream() from Bronze
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spark Streaming Job #2 (streaming_silver.py)           â”‚
â”‚  - Bronze â†’ Silver transformations                      â”‚
â”‚  - Cleaning, enrichment, aggregations                   â”‚
â”‚  - Additional calculations                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ writeStream() to Silver
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Delta Lake Silver Layer (/tmp/delta/silver/)           â”‚
â”‚  - Curated, high-quality data                           â”‚
â”‚  - Format: Parquet + Delta transactions                 â”‚
â”‚  - Ready for analytics and BI tools                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ SELECT * FROM Delta tables
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit Dashboard (http://localhost:8501)            â”‚
â”‚  - Reads Bronze & Silver layers                         â”‚
â”‚  - 5-second cache for freshness                         â”‚
â”‚  - Real-time metrics and visualizations                 â”‚
â”‚  - Auto-refresh every 5-30 seconds (configurable)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ WebSocket
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web Browser (http://localhost:8501)                    â”‚
â”‚  - Interactive dashboard                                â”‚
â”‚  - Live charts (Plotly)                                 â”‚
â”‚  - System status monitoring                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Quick Start

### 1. Prerequisites
```bash
# Required software already installed:
âœ… Java 11 (OpenJDK)
âœ… Python 3.10
âœ… Kafka 3.7.1
âœ… Spark 3.5.0
âœ… Delta Lake 3.2.0
```

### 2. Start Everything
```bash
cd /home/ismail/projects/spark_streaming_kafka
bash start_pipeline.sh
```

This will:
- âœ… Start Zookeeper (port 2181)
- âœ… Start Kafka Broker (port 9092)
- âœ… Create topic `ventes_stream`
- âœ… Start Producer (generates messages)
- âœ… Start Delta job (Kafka â†’ Bronze)
- âœ… Start Silver job (Bronze â†’ Silver)
- âœ… Start Streamlit dashboard (port 8501)

### 3. Access Dashboard
```
Open browser: http://localhost:8501
```

---

## ğŸ“Š Dashboard Walkthrough

### Home Tab: Metrics
Shows 5 real-time KPIs:
- **Total Records**: All messages processed (increases by 1-2 per second)
- **Unique Clients**: Count of distinct customer IDs
- **Unique Products**: Count of distinct product SKUs
- **Total Revenue**: Sum of all transaction amounts
- **Avg Transaction**: Mean transaction size

### Charts Tab 1: Sales by Country
- Bar chart with country names (x-axis)
- Total revenue per country (y-axis)
- Updates as data flows through
- Hover for exact values

### Charts Tab 2: Top 10 Products
- Ranked product names
- Sales volume (quantity sold)
- Auto-ranks as new data arrives
- Shows product performance

### Charts Tab 3: Sales Timeline
- Date/time on x-axis
- Cumulative sales count on y-axis
- Shows sales velocity trends
- Useful for detecting anomalies

### Data Tab: Raw Records
- Display of last 100 records from Bronze
- All columns visible (vente_id, client_id, montant, timestamp, etc.)
- Searchable and sortable
- Useful for data validation

### Config Tab: Settings
- **Refresh Rate**: 5-30 seconds
- **System Architecture**: View pipeline diagram
- **Monitoring**: Copy-paste log commands
- **Help**: Troubleshooting guide

---

## ğŸ“ File Structure

```
/home/ismail/projects/spark_streaming_kafka/
â”œâ”€â”€ producer_ventes.py              # Kafka producer (generates messages)
â”œâ”€â”€ spark_streaming_delta.py         # Spark job: Kafka â†’ Bronze
â”œâ”€â”€ streaming_silver.py              # Spark job: Bronze â†’ Silver
â”œâ”€â”€ streamlit_dashboard.py           # Streamlit web app (THIS FILE)
â”œâ”€â”€ query_utils.py                   # Helper functions
â”œâ”€â”€ setup_utils.py                   # Setup utilities
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ config.ini                       # Configuration file
â”œâ”€â”€ start_pipeline.sh                # Start all jobs (one command!)
â”œâ”€â”€ check_status.sh                  # Verify system status
â”œâ”€â”€ STREAMLIT_GUIDE.md               # This dashboard guide
â”œâ”€â”€ README.md                        # Project overview
â”œâ”€â”€ QUICKSTART.md                    # Getting started
â””â”€â”€ /tmp/delta/
    â”œâ”€â”€ bronze/ventes_stream/        # Raw data (Kafka â†’ Bronze)
    â”œâ”€â”€ silver/                      # Transformed data (Bronze â†’ Silver)
    â””â”€â”€ checkpoints/                 # Spark checkpoints
```

---

## ğŸ” Monitoring & Debugging

### Check System Status
```bash
bash check_status.sh
```

Shows:
- âœ… All services running
- ğŸ“Š Data directory sizes
- ğŸ“ˆ Message flow statistics
- ğŸ”— Links to monitoring commands

### View Live Logs

**Producer (messages being generated)**
```bash
tail -f /tmp/producer.log
```

**Delta Job (Kafka ingestion)**
```bash
tail -f /tmp/spark_delta.log
```

**Silver Job (transformations)**
```bash
tail -f /tmp/spark_silver.log
```

**Streamlit (dashboard)**
```bash
tail -f /tmp/streamlit.log
```

### Query Data Directly

**Check Bronze data**
```bash
spark-sql --master local[2] \
  --packages io.delta:delta-spark_2.12:3.2.0 \
  -e "SELECT COUNT(*) FROM delta.\`/tmp/delta/bronze/ventes_stream\`"
```

**Check Silver data**
```bash
spark-sql --master local[2] \
  --packages io.delta:delta-spark_2.12:3.2.0 \
  -e "SELECT COUNT(*) FROM delta.\`/tmp/delta/silver\`"
```

---

## ğŸ›‘ Stop All Services

### Stop Gracefully
```bash
pkill -f 'producer_ventes|spark_streaming|streamlit'
```

### Stop Kafka
```bash
cd /home/ismail/apps/kafka_2.13-3.7.1
bin/kafka-server-stop.sh
bin/zookeeper-server-stop.sh
```

### Full Cleanup (if needed)
```bash
# Stop all processes
pkill -f 'producer|spark_streaming|streamlit|kafka|zookeeper'

# Delete data (âš ï¸ caution!)
rm -rf /tmp/delta/
rm -f /tmp/*.log

# Recreate and restart
bash start_pipeline.sh
```

---

## ğŸ› Troubleshooting

### Dashboard Not Loading
```bash
# Check if running
ps aux | grep streamlit

# Check port
lsof -i :8501

# Restart
pkill -f streamlit
cd /home/ismail/projects/spark_streaming_kafka
python3 -m streamlit run streamlit_dashboard.py --server.headless=true < /dev/null &
```

### No Data in Dashboard
```bash
# Check producer is generating messages
tail -f /tmp/producer.log | grep -i "message\|error"

# Check Delta job is ingesting
tail -f /tmp/spark_delta.log | grep -i "offset\|batch"

# Check Bronze data exists
ls -lh /tmp/delta/bronze/ventes_stream/ | head -5
```

### Kafka Not Starting
```bash
# Check Zookeeper is running
ps aux | grep zookeeper | grep -v grep

# Clean Kafka data (âš ï¸ deletes everything)
rm -rf /home/ismail/apps/kafka_2.13-3.7.1/data-dir
pkill -f kafka
pkill -f zookeeper

# Restart
cd /home/ismail/apps/kafka_2.13-3.7.1
bin/zookeeper-server-start.sh config/zookeeper.properties &
sleep 3
bin/kafka-server-start.sh config/server.properties &
```

---

## ğŸ“ˆ Performance & Scaling

### Current Configuration
- **Producer Rate**: ~30 messages/minute (1 every 2 seconds)
- **Kafka Partitions**: 1 (single partition)
- **Spark Workers**: 2 (local[2])
- **Memory**: 1GB Kafka, 2GB Spark
- **Refresh Rate**: 5 seconds (configurable 5-30)

### Expected Data Growth
| Time | Total Records | Bronze Files | Silver Files |
|------|---------------|--------------|--------------|
| 1 min | ~30 | 1-3 | - |
| 5 min | ~150 | 5-10 | 1-2 |
| 10 min | ~300 | 10-20 | 2-5 |
| 1 hour | ~1,800 | 50-100 | 10-20 |
| 1 day | ~43,200 | 1000+ | 200+ |

### Scaling Tips
1. **Increase Producer Rate**: Edit `producer_ventes.py` (change `sleep(2)` to `sleep(0.5)`)
2. **Add Kafka Partitions**: Run `start_pipeline.sh` with modified topic creation
3. **Scale Spark**: Change `local[2]` to `local[4]` or `local[*]`
4. **Increase Memory**: Modify Spark configuration in job files

---

## ğŸ“ Learning Resources

### Understanding the Pipeline
1. **Producer** - How random sales data is generated
2. **Kafka** - Message broker and topic management
3. **Spark Streaming** - Watermarking and batching logic
4. **Delta Lake** - ACID transactions and versioning
5. **Streamlit** - Web framework for data apps

### Key Concepts
- **Kafka Topic**: Named stream of messages (like a table)
- **Partition**: Parallel unit within a topic
- **Batch**: Micro-batch of messages processed together
- **Watermark**: Late data cutoff threshold
- **Delta Lake**: Data lake format with ACID transactions
- **Bronze/Silver/Gold**: Data quality layers
  - **Bronze**: Raw, unprocessed data (1:1 from source)
  - **Silver**: Cleaned, deduplicated, consistent schema
  - **Gold**: Aggregated, business-ready analytics

---

## âœ… Success Checklist

- [ ] Zookeeper running (port 2181)
- [ ] Kafka running (port 9092)
- [ ] Producer generating messages
- [ ] Delta job ingesting to Bronze
- [ ] Silver job processing data
- [ ] Streamlit dashboard accessible (port 8501)
- [ ] Real-time metrics updating
- [ ] Charts displaying data
- [ ] Refresh rate configurable
- [ ] All logs accessible

---

## ğŸ¯ Next Steps

1. **Access Dashboard**: Open http://localhost:8501
2. **Monitor Metrics**: Watch KPIs update in real-time
3. **Explore Data**: View raw transactions in Data tab
4. **Check Logs**: Monitor pipeline health with tail commands
5. **Experiment**: Change refresh rate, generate more data
6. **Scale**: Increase producer rate and Kafka partitions

---

## ğŸ“ Support

### Common Issues

**Dashboard Crashes**
```bash
pkill -f streamlit
python3 -m streamlit run streamlit_dashboard.py --server.headless=true < /dev/null &
```

**No New Data**
```bash
# Restart entire pipeline
bash start_pipeline.sh
```

**Out of Disk Space**
```bash
# Check storage
df -h /tmp

# Clear old data
rm -rf /tmp/delta/bronze
# Restart pipeline to rebuild
```

---

## ğŸ† System Status

**Last Verified**: 2025-12-12 14:40+00:00

âœ… **All Components Operational**
- Zookeeper: Running
- Kafka: Running
- Producer: Generating messages
- Delta Job: Ingesting to Bronze
- Silver Job: Processing Bronze â†’ Silver
- Dashboard: Live on http://localhost:8501

ğŸ“Š **Real-Time Metrics Available**
- Total Records, Unique Clients, Products, Revenue, Avg Transaction
- 4 visualization charts with auto-refresh
- Raw data browser with 100-record pagination
- Configurable refresh rate (5-30 seconds)

ğŸ¯ **Ready for Use**
Open your browser and navigate to: **http://localhost:8501**

---

**Status**: âœ¨ **PRODUCTION READY** âœ¨

All infrastructure deployed and operational. Dashboard displaying live streaming data from Kafka pipeline with real-time visualizations.
