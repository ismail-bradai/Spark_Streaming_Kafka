# üèóÔ∏è Architecture & Design Document

## System Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LAKEHOUSE ARCHITECTURE                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

INGESTION LAYER
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Sales Producer  ‚îÇ  ‚óÑ‚îÄ‚îÄ Generates ~30 sales/minute
    ‚îÇ  (Simulator)     ‚îÇ     Random clients, products, quantities
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº JSON messages
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Kafka Topic      ‚îÇ  ‚óÑ‚îÄ‚îÄ Distributed, partitioned queue
    ‚îÇ ventes_stream    ‚îÇ     Offset tracking, replay capability
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ

BRONZE LAYER (Raw Data)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Spark Streaming Consumer    ‚îÇ
    ‚îÇ  - Read from Kafka           ‚îÇ
    ‚îÇ  - Parse JSON schema         ‚îÇ
    ‚îÇ  - Watermark late data (10m) ‚îÇ
    ‚îÇ  - Add metadata              ‚îÇ
    ‚îÇ  - Partition by date         ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº Append mode
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Delta Lake Bronze Table     ‚îÇ
    ‚îÇ  /tmp/delta/bronze/          ‚îÇ
    ‚îÇ                              ‚îÇ
    ‚îÇ  Partitions: jour (YYYY-MM-DD)
    ‚îÇ  Format: Parquet + Delta Log ‚îÇ
    ‚îÇ  Retention: 90 days (adjust) ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

SILVER LAYER (Aggregations)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Aggregation Jobs (Batch)    ‚îÇ
    ‚îÇ  - Read Bronze               ‚îÇ
    ‚îÇ  - Group & aggregate         ‚îÇ
    ‚îÇ  - Calculate metrics         ‚îÇ
    ‚îÇ  - Detect patterns           ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ          ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇClient ‚îÇ  ‚îÇCountry‚îÇ     ‚îÇ   Segment    ‚îÇ
         ‚îÇAgg    ‚îÇ  ‚îÇAgg    ‚îÇ     ‚îÇ   Agg        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ          ‚îÇ                ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ SLV1  ‚îÇ  ‚îÇ SLV2  ‚îÇ     ‚îÇ    SLV3      ‚îÇ
         ‚îÇ/agg/  ‚îÇ  ‚îÇ/pays/ ‚îÇ     ‚îÇ  /segment/   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

GOLD LAYER (Analytics & BI)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  Dashboard & Analytics Query    ‚îÇ
         ‚îÇ  - Top customers               ‚îÇ
         ‚îÇ  - Revenue trends              ‚îÇ
         ‚îÇ  - Geographic analysis         ‚îÇ
         ‚îÇ  - Loyalty metrics             ‚îÇ
         ‚îÇ  - Segmentation results        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Data Flow Diagram

```
Time Dimension: Real-time ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Near real-time ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Batch Analytics

    Every 2 sec                Every micro-batch        Periodic (hourly/daily)
         ‚îÇ                           ‚îÇ                          ‚îÇ
         ‚ñº                           ‚ñº                          ‚ñº
    Producer                    Streaming Job              Analytics Job
    (sends 1 sale)        (consumes 10-20 sales)      (aggregates 1000+ sales)
         ‚îÇ                           ‚îÇ                          ‚îÇ
         ‚îî‚îÄ‚îÄ‚ñ∫ Kafka ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Bronze ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Checkpoints             ‚îÇ
                 (queue)    (history)   (fault-tolerance)       ‚îÇ
                                                                 ‚îÇ
                                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                            ‚îÇ
                                            ‚ñº
                                       Silver Tables
                                      (aggregations)
                                            ‚îÇ
                                            ‚ñº
                                      SQL / Dashboard
```

## Component Details

### 1. Producer (producer_ventes.py)

**Purpose**: Simulate real-world sales events

**Key Features**:
- Generates random but realistic sales data
- 5 products √ó 5 clients √ó random quantities
- JSON serialization for Kafka
- Async callbacks for delivery confirmation
- Configurable batch interval

**Data Structure**:
```python
{
    "vente_id": 1,                              # Unique transaction ID
    "client_id": 1,                             # Customer reference
    "produit_id": 101,                          # Product reference
    "timestamp": "2024-12-12T10:15:23.123456",  # Event time (ISO-8601)
    "quantite": 2,                              # Number of units
    "montant": 1799.98,                         # Revenue (EUR)
    "client_nom": "Jean Dupont",                # Customer name
    "produit_nom": "Ordinateur portable",       # Product name
    "categorie": "√âlectronique",                # Product category
    "pays": "France",                           # Customer country
    "segment": "Particulier"                    # B2B or B2C
}
```

**Execution**: `python producer_ventes.py`

### 2. Streaming Consumer (spark_streaming_delta.py)

**Purpose**: Real-time ingestion and Bronze layer creation

**Architecture**:
```
Kafka Source
    ‚Üì readStream
JSON Parser (from_json)
    ‚Üì
Data Enrichment (timestamp conversion, metadata)
    ‚Üì withWatermark (10 minutes)
Watermarked DataFrame
    ‚Üì writeStream
Delta Lake (Bronze)
```

**Key Transformations**:
1. **Schema Parsing**: Convert JSON string to structured data
2. **Timestamp Conversion**: Parse ISO-8601 to Spark timestamp
3. **Metadata Addition**:
   - `date_ingestion`: Current processing time
   - `jour`: Date partition key (YYYY-MM-DD)
4. **Watermarking**: Allows 10-minute late data
5. **Partitioning**: Organized by `jour` for query efficiency

**Checkpointing**:
- Location: `/tmp/delta/checkpoints/ventes_bronze`
- Tracks: Kafka offsets, source metadata
- Recovery: Resumes from last successful batch if interrupted

**Execution**:
```bash
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-spark_2.12:3.2.0 \
  spark_streaming_delta.py
```

**Metrics Tracked**:
- Input Rows: Records read from Kafka
- Output Rows: Records written to Delta
- Batch Duration: Time to process batch
- Processing Latency: Total end-to-end delay

### 3. Analytics Pipeline (streaming_silver.py)

**Purpose**: Aggregate Bronze data and create analytical views

**Three Silver Tables**:

#### a) Client Aggregation (ventes_aggreges)
```sql
GROUP BY client_id, client_nom, pays, segment, jour
AGGREGATE:
  - sum(quantite)           ‚Üí total_quantite
  - sum(montant)            ‚Üí total_depense
  - count(*)                ‚Üí nb_achats
  - avg(montant)            ‚Üí panier_moyen
  - min(montant)            ‚Üí achat_min
  - max(montant)            ‚Üí achat_max
  - count(distinct produit) ‚Üí nb_produits_distincts
CALCULATED:
  - est_client_fidele (nb_achats >= 2)
  - segment_depense (Premium/Standard/Economy)
```

#### b) Country Analysis (ventes_par_pays)
```sql
GROUP BY pays
AGGREGATE:
  - count(*)                    ‚Üí total_transactions
  - sum(montant)                ‚Üí revenue_total
  - avg(montant)                ‚Üí transaction_moyenne
  - sum(quantite)               ‚Üí items_sold
  - count(distinct client_id)   ‚Üí unique_customers
```

#### c) Segment Analysis (ventes_par_segment)
```sql
GROUP BY segment
AGGREGATE:
  - count(*)                    ‚Üí total_transactions
  - sum(montant)                ‚Üí revenue_total
  - avg(montant)                ‚Üí transaction_moyenne
  - count(distinct client_id)   ‚Üí unique_customers
  - count(distinct pays)        ‚Üí countries_count
```

**Execution**: Run periodically (hourly/daily)
```bash
spark-submit \
  --packages io.delta:delta-spark_2.12:3.2.0 \
  streaming_silver.py
```

## Watermarking Deep Dive

**Problem**: Late-arriving data in distributed systems
- Records delayed by network issues
- Duplicate handling needed
- Out-of-order processing

**Solution**: 10-minute watermark
```
Event Time: 10:00:00
Watermark: 10:10:00 (current_watermark + delay)

Events arriving:
- 10:08:00 ‚úÖ Before watermark ‚Üí Process
- 10:12:00 ‚ùå After watermark ‚Üí Drop (late)
```

**Configuration**: Edit `WATERMARK_DELAY` in `spark_streaming_delta.py`

## Partitioning Strategy

**Bronze Layer**: Partitioned by `jour` (date)
```
/tmp/delta/bronze/ventes_stream/
‚îú‚îÄ‚îÄ jour=2024-12-10/
‚îÇ   ‚îú‚îÄ‚îÄ part-00000.parquet
‚îÇ   ‚îî‚îÄ‚îÄ part-00001.parquet
‚îú‚îÄ‚îÄ jour=2024-12-11/
‚îÇ   ‚îî‚îÄ‚îÄ part-00000.parquet
‚îú‚îÄ‚îÄ jour=2024-12-12/
‚îÇ   ‚îú‚îÄ‚îÄ part-00000.parquet
‚îÇ   ‚îî‚îÄ‚îÄ _delta_log/
‚îÇ       ‚îú‚îÄ‚îÄ 00000000000000000000.json
‚îÇ       ‚îî‚îÄ‚îÄ 00000000000000000001.json
```

**Benefits**:
- Faster date-range queries
- Easier data retention policies
- Parallel partition pruning
- Simple archive/delete by date

## Checkpoint Mechanism

**Location**: `/tmp/delta/checkpoints/ventes_bronze/`

**Contents**:
- Kafka offset tracking
- Processing state
- UUID checkpoint files

**Recovery**:
1. Stream stops unexpectedly
2. Spark reads checkpoint metadata
3. Resumes from last committed offset
4. No duplicate data (exactly-once semantics)

## Performance Considerations

### 1. Throughput
- **Current**: ~30 sales/minute from producer
- **Spark Processing**: Sub-second latency per batch
- **Bottleneck**: Kafka broker (can handle 100K+ msgs/sec)

### 2. Scalability
- **Increase Partitions**: `--partitions 4` for Kafka topic
- **Parallel Consumers**: Run multiple Spark jobs
- **Cluster Mode**: Deploy on Spark cluster

### 3. Storage
- **Compression**: Delta automatically compresses
- **Retention**: Vacuum old versions after 24 hours
- **Estimate**: ~1KB per transaction, ~1GB per 1M sales

### 4. Query Performance
- **Partitioning**: Queries by `jour` are fast
- **Caching**: Silver tables cached after first read
- **Optimization**: Spark auto-optimizes via Catalyst

## Fault Tolerance

### Kafka Failures
- **Rebalancing**: Auto-detected, seamless recovery
- **Broker Restart**: Offset replay from checkpoint
- **Data Loss**: One partition replication factor (adjust for production)

### Spark Failures
- **Task Failures**: Automatic retry (max 3 attempts)
- **Driver Failure**: Checkpoint recovery
- **Network**: Timeout and reconnect logic

### Delta Lake
- **ACID Guarantees**: Automatic conflict resolution
- **Version History**: Always available for rollback
- **Concurrent Writes**: Handled with optimistic locking

## Security Considerations

### Current Setup (Local Development)
‚ö†Ô∏è Not production-ready:
- No Kafka authentication
- No SSL/TLS encryption
- No data encryption at rest
- All data in `/tmp` (no persistence)

### Production Hardening
1. **Kafka Security**:
   - Enable SASL/SCRAM authentication
   - Use SSL/TLS for transport encryption

2. **Data Security**:
   - Encrypt Delta Lake with KMS
   - Use network VPN for Spark cluster
   - Enable audit logging

3. **Access Control**:
   - Use Delta Lake table ACLs
   - Implement Spark authentication
   - Role-based data access

4. **Compliance**:
   - GDPR: Data retention policies
   - Audit: Track all data modifications
   - Backup: Regular snapshots

## Monitoring & Observability

### Metrics Available

**From `query.recentProgress`**:
```python
{
    "numInputRows": 15,              # Records from Kafka
    "numOutputRows": 15,             # Records to Delta
    "numUpdatedStateRows": 0,        # State updates
    "durationMs": {
        "addBatch": 245,             # Processing time
        "commitOffsets": 12,
        "getBatch": 5
    },
    "eventTime": {
        "avg": "2024-12-12T10:15:30Z",
        "max": "2024-12-12T10:15:32Z",
        "min": "2024-12-12T10:15:25Z"
    },
    "states": [{
        "numRowsTotal": 1000,
        "numRowsUpdated": 15
    }]
}
```

**Custom Dashboards**:
- Kafka lag monitoring
- Delta Lake write latency
- Query execution times
- System resource usage (CPU, memory, I/O)

## Disaster Recovery

### Backup Strategy
```bash
# Backup Bronze table (daily)
cp -r /tmp/delta/bronze /backups/delta_bronze_$(date +%Y%m%d)

# Backup checkpoints
cp -r /tmp/delta/checkpoints /backups/checkpoints_$(date +%Y%m%d)
```

### Recovery Procedures
1. **Table Corruption**: Restore from backup, restart pipeline
2. **Data Loss**: Use Delta Lake time travel
   ```python
   spark.read.format("delta") \
       .option("timestampAsOf", "2024-12-12") \
       .load("/tmp/delta/bronze/...")
   ```
3. **Complete Failure**: Replay from Kafka (offset tracking)

---

## Next Enhancements

- [ ] Schema evolution and management
- [ ] Advanced data quality validations
- [ ] Real-time anomaly detection
- [ ] Machine learning feature engineering
- [ ] Cloud deployment (AWS S3, Azure ADLS)
- [ ] Multi-cluster federation
- [ ] Advanced monitoring dashboard

---

**Architecture Version**: 1.0  
**Last Updated**: December 2024  
**Maintainer**: Data Engineering Team
