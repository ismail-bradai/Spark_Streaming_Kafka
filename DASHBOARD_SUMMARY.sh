#!/bin/bash
# ðŸ“Š STREAMLIT DASHBOARD - FINAL SUMMARY & QUICK REFERENCE

cat << 'EOF'

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                             â•‘
â•‘         ðŸš€ SPARK STREAMING KAFKA + STREAMLIT DASHBOARD ðŸš€                  â•‘
â•‘                      Complete Real-Time Visualization                       â•‘
â•‘                                                                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


âœ¨ QUICK START (One Command!)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  1ï¸âƒ£  Start Everything:
      cd /home/ismail/projects/spark_streaming_kafka
      bash start_pipeline.sh

  2ï¸âƒ£  Open Dashboard:
      http://localhost:8501

  3ï¸âƒ£  Monitor Progress:
      bash check_status.sh


ðŸŒ LIVE DASHBOARD
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   âœ¨ URL: http://localhost:8501
   
   ðŸ“ˆ Real-Time Metrics
      â€¢ Total Records (cumulative)
      â€¢ Unique Clients (distinct count)
      â€¢ Unique Products (SKU count)
      â€¢ Total Revenue (sum)
      â€¢ Avg Transaction (mean)
   
   ðŸ“Š Interactive Charts
      â€¢ Sales by Country (bar)
      â€¢ Top 10 Products (ranking)
      â€¢ Sales Timeline (line)
      â€¢ Raw Data Browser (table)
   
   âš™ï¸  Configuration
      â€¢ Auto-refresh rate (5-30 seconds)
      â€¢ System architecture
      â€¢ Monitoring commands
      â€¢ Help & troubleshooting


ðŸ“¡ SYSTEM COMPONENTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Zookeeper ........... Port 2181 (Kafka coordinator)
   Kafka Broker ........ Port 9092 (Message broker)
   Spark Jobs .......... Local[2] (2 workers)
   Delta Lake .......... /tmp/delta/ (ACID storage)
   Streamlit ........... Port 8501 (Web dashboard)
   
   Producer ............ python3 producer_ventes.py
   Delta Job ........... spark_streaming_delta.py
   Silver Job .......... streaming_silver.py


ðŸ” MONITORING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   View All Logs:
      tail -f /tmp/producer.log        # Messages generated
      tail -f /tmp/spark_delta.log     # Kafka ingestion
      tail -f /tmp/spark_silver.log    # Transformations
      tail -f /tmp/streamlit.log       # Dashboard

   Check Status:
      bash check_status.sh             # Full system health

   Query Data:
      spark-sql --master local[2] \\
        --packages io.delta:delta-spark_2.12:3.2.0 \\
        -e "SELECT COUNT(*) FROM delta.\`/tmp/delta/bronze/ventes_stream\`"


ðŸ›‘ STOP ALL JOBS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   pkill -f 'producer_ventes|spark_streaming|streamlit'


ðŸ“ PROJECT FILES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Core Streaming Jobs:
      ðŸ“œ producer_ventes.py       - Generates sales messages
      ðŸ“œ spark_streaming_delta.py - Kafka â†’ Delta Bronze
      ðŸ“œ streaming_silver.py      - Bronze â†’ Silver (transformations)

   Visualization:
      ðŸ“œ streamlit_dashboard.py   - Interactive web dashboard
      ðŸ“œ STREAMLIT_GUIDE.md       - Dashboard documentation
      ðŸ“œ DASHBOARD_README.md      - Complete guide

   Utilities:
      ðŸ“œ query_utils.py           - Query helpers
      ðŸ“œ setup_utils.py           - Setup functions
      ðŸ“œ requirements.txt         - Python dependencies
      ðŸ“œ config.ini               - Configuration

   Scripts:
      ðŸ”§ start_pipeline.sh        - Start entire system
      ðŸ”§ check_status.sh          - Verify system status
      ðŸ”§ check_setup.sh           - Verify setup


ðŸ“Š DATA FLOW DIAGRAM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Producer (python3)
        â†“ 1 message every 2 seconds
   Kafka Topic: ventes_stream
        â†“ Kafka Consumer
   Spark Delta Job (reads Kafka)
        â†“ writeStream()
   Delta Lake Bronze (/tmp/delta/bronze/)
        â†“ readStream()
   Spark Silver Job (transformations)
        â†“ writeStream()
   Delta Lake Silver (/tmp/delta/silver/)
        â†“ Spark SQL Queries
   Streamlit Dashboard (http://localhost:8501)
        â†“ Auto-refresh every 5-30 seconds
   Web Browser Visualization


âš¡ PERFORMANCE STATS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Message Rate:        ~30/minute (1 every 2 seconds)
   Kafka Partitions:    1
   Spark Workers:       2 (local[2])
   Memory (Kafka):      1 GB
   Memory (Spark):      2 GB
   Refresh Rate:        5-30 seconds (configurable)
   
   Expected Growth:
      1 minute:   ~30 records
      5 minutes:  ~150 records
      1 hour:     ~1,800 records
      1 day:      ~43,200 records


âœ… SUCCESS CHECKLIST
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   [ ] Dashboard loads on http://localhost:8501
   [ ] Metrics update in real-time
   [ ] Charts display data correctly
   [ ] Raw data tab shows transactions
   [ ] Refresh rate is adjustable
   [ ] All logs are accessible
   [ ] Producer messages flowing
   [ ] Delta job ingesting data
   [ ] Silver job processing data


ðŸŽ¯ NEXT STEPS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   1. Access Dashboard
      âžœ Open http://localhost:8501 in your browser

   2. Monitor System
      âžœ Run: bash check_status.sh
      âžœ Watch logs: tail -f /tmp/producer.log

   3. Explore Data
      âžœ Click on "Raw Data" tab to see transactions
      âžœ Scroll through sales records
      âžœ View data structure

   4. Observe Metrics
      âžœ Watch KPIs update every 5 seconds
      âžœ Notice records accumulating
      âžœ Track revenue growth

   5. Experiment
      âžœ Adjust refresh rate (sidebar)
      âžœ Increase producer rate (edit producer_ventes.py)
      âžœ Scale Spark workers


ðŸ“š DOCUMENTATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   START_HERE.txt      - First steps
   QUICKSTART.md       - Quick setup guide
   README.md           - Project overview
   ARCHITECTURE.md     - System design
   DASHBOARD_README.md - Dashboard documentation
   STREAMLIT_GUIDE.md  - Dashboard features & troubleshooting
   TROUBLESHOOTING.md  - Common issues & solutions


ðŸ› TROUBLESHOOTING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Dashboard not loading?
      pkill -f streamlit
      cd /home/ismail/projects/spark_streaming_kafka
      python3 -m streamlit run streamlit_dashboard.py --server.headless=true < /dev/null &

   No data showing?
      â€¢ Check producer: tail -f /tmp/producer.log
      â€¢ Check Delta job: tail -f /tmp/spark_delta.log
      â€¢ Check Bronze data: ls -la /tmp/delta/bronze/ventes_stream/

   Need fresh start?
      pkill -f 'producer|spark_streaming|streamlit'
      rm -rf /tmp/delta/
      bash start_pipeline.sh


ðŸŽ“ LEARNING
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Understand the Pipeline:
      1. Producer creates messages (JSON format)
      2. Kafka receives and stores messages
      3. Spark reads from Kafka (streaming)
      4. Delta Lake ACID writes to Bronze
      5. Silver job transforms Bronze data
      6. Streamlit queries Delta for visualization

   Key Concepts:
      â€¢ Kafka Topic: Named stream (like a table)
      â€¢ Partition: Parallel processing unit
      â€¢ Batch: Micro-batch of messages
      â€¢ Watermark: Late data cutoff
      â€¢ Delta Lake: Data lake with ACID
      â€¢ Bronze/Silver: Data quality layers


ðŸš€ PRODUCTION DEPLOYMENT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

   Current Setup:
      âœ… Development environment (local machine)
      âœ… Single Kafka broker (1 partition)
      âœ… Local Spark (2 workers)
      âœ… Streamlit dashboard (web UI)

   To Scale:
      â€¢ Increase Kafka partitions (parallel ingestion)
      â€¢ Add Spark workers (distributed processing)
      â€¢ Increase producer rate (higher throughput)
      â€¢ Use cloud Kafka (AWS MSK, Confluent Cloud)
      â€¢ Deploy to Kubernetes (containerized)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                    ðŸŽ‰ YOU'RE ALL SET! ðŸŽ‰
                    
         Your real-time streaming analytics pipeline is ready!
         
         Open http://localhost:8501 in your browser now!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EOF
